{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aace42d2-4a81-46f7-8543-05a084418e50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b5de252-791d-4e3f-aa89-e44dc1a8232f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### pip install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a590cf2-ec94-40e7-827f-236e56f41ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install\n",
    "!pip install faiss-gpu --quiet\n",
    "!pip install boto3 --quiet\n",
    "!pip install langchain-aws --quiet\n",
    "!pip install langchain-community boto3 --quiet\n",
    "!pip install boto3 requests requests-aws4auth --quiet\n",
    "!pip install pymongo --quiet\n",
    "!pip install google-search-results sagemaker --quiet\n",
    "!pip install boto3 nltk --quiet\n",
    "!pip install wikipedia --quiet\n",
    "!pip install tavily-python --quiet\n",
    "!pip install --upgrade langchain sympy numexpr --quiet\n",
    "!pip install langchain-experimental --quiet\n",
    "!pip install -U langgraph langchain_community langchain_anthropic langchain_experimental --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d19e77-56b1-4dcb-85fd-15a35da41b4c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4080bd93-2421-4b53-b152-33587505ccb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "from langchain import hub\n",
    "\n",
    "#chat models\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "\n",
    "#LLMs\n",
    "from langchain_aws import BedrockLLM\n",
    "from langchain_community.llms import AmazonAPIGateway\n",
    "from langchain_aws import SagemakerEndpoint\n",
    "\n",
    "#prompts\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#embedding models\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "# from langchain_community.embeddings import BedrockEmbeddings\n",
    "from langchain_community.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain_community.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "\n",
    "#document loaders\n",
    "from langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader\n",
    "from langchain_community.document_loaders import AmazonTextractPDFLoader\n",
    "\n",
    "#vector stores\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from langchain_community.vectorstores import DocumentDBVectorSearch\n",
    "\n",
    "from langchain_aws.vectorstores.inmemorydb import InMemoryVectorStore\n",
    "\n",
    "#retrievers\n",
    "from langchain_aws import AmazonKendraRetriever\n",
    "from langchain_aws import AmazonKnowledgeBasesRetriever\n",
    "\n",
    "#memory\n",
    "from langchain_community.chat_message_histories import DynamoDBChatMessageHistory\n",
    "\n",
    "#graphs\n",
    "from langchain_community.graphs import NeptuneGraph\n",
    "from langchain_community.graphs import NeptuneAnalyticsGraph\n",
    "from langchain_community.chains.graph_qa.neptune_cypher import NeptuneOpenCypherQAChain\n",
    "\n",
    "from langchain_community.graphs import NeptuneRdfGraph\n",
    "from langchain_community.chains.graph_qa.neptune_sparql import NeptuneSparqlQAChain\n",
    "\n",
    "#callbacks\n",
    "from langchain_community.callbacks.bedrock_anthropic_callback import BedrockAnthropicTokenUsageCallbackHandler\n",
    "from langchain_community.callbacks import SageMakerCallbackHandler\n",
    "\n",
    "#chains\n",
    "# from langchain_experimental.comprehend_moderation import AmazonComprehendModerationChain\n",
    "\n",
    "#tavily\n",
    "from tavily import TavilyClient\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "#tools\n",
    "from langchain.agents import initialize_agent, Tool, load_tools\n",
    "from langchain.tools import BaseTool\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import DirectoryLoader, CSVLoader\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain_core.tools import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "#agent\n",
    "from langchain.agents import create_tool_calling_agent, AgentType\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e41a72-31d6-411f-b29d-f08db9e60542",
   "metadata": {
    "tags": []
   },
   "source": [
    "## setup agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9431705c-f5c9-4474-b78d-b06ad5440d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_parameter = {\"temperature\": 0.9, \"top_p\": .5, \"max_tokens_to_sample\": 200}\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "bedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3.client('bedrock'),\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25399568-bcb4-488c-acd2-329da4fd4119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embeddings = BedrockEmbeddings(\n",
    "    client=boto3.client('bedrock-runtime', region_name=\"us-east-1\"),\n",
    "    model_id = \"amazon.titan-embed-text-v2:0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31162b-da4d-4634-b018-b53837345bfd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### initiate tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86cad17c-4cba-4009-bcfe-a03488898d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n",
    "search = TavilySearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da0cd621-7749-4c69-8f7d-a4cc68f97b87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the path to the directory containing CSV files\n",
    "data_files_directory = \"data_files\"\n",
    "\n",
    "# Use DirectoryLoader to load all CSV files\n",
    "loader = DirectoryLoader(\n",
    "    data_files_directory,\n",
    "    glob=\"**/*.csv\",  # Match all CSV files in the directory (including subdirectories)\n",
    "    loader_cls=CSVLoader  # Use CSVLoader for parsing the files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "461c185f-aefe-42ea-a2e1-0c3fedc92c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200\n",
    ").split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector.as_retriever()\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"Retriever_tool\",\n",
    "    \"First search internal data files to answer any queries. If there is no useful information then use other tools.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "662ad84e-7555-4277-8980-ef8788aa9fed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prompt = PromptTemplate(\n",
    "#     input_variables=[\"input\", \"agent_scratchpad\"],  # Include both input and agent_scratchpad\n",
    "#     template=\"\"\"\n",
    "# Use the retriever tool to answer any queries related to internal data files. \n",
    "# Only use external tools (like Wikipedia) if no relevant information is found internally.\n",
    "# When asked about a specific item output the answers to the following queries: \n",
    "\n",
    "# Should this item be ordered? \n",
    "# Use recent stock statistics to see if the stocks for this month are low compared to how much was used last month.\n",
    "# Use the statistics from last year to predict how much of each item will be needed for the rest of this month and next month.\n",
    "# Use what month it is at the time the call is being made and whether any important holidays eg christmas or summer will be coming up and use this information in your decision.\n",
    "\n",
    "# What brand?\n",
    "# Use the customers reviews to decide this. \n",
    "\n",
    "# If this item should be ordered, how much of this item should be ordered?\n",
    "# Use the repl tool to calculate these numbers.\n",
    "\n",
    "# In your final output:\n",
    "# 1. Explain your reasoning for whether an item should be ordered and how much.\n",
    "# Don't explain your reasoning for\n",
    "# Explain the maths you used.\n",
    "# 2. Draft an email to an imaginary supplier of this brand to order the number of the item you think should be ordered, signing off as Reply Auto Replenishment.\n",
    "\n",
    "# Query: {input}\n",
    "\n",
    "# {agent_scratchpad}\n",
    "# \"\"\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "838aeb17-ce6d-4659-90e4-6b3acc7fd596",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"agent_scratchpad\"],  # Include both input and agent_scratchpad\n",
    "    template=\"\"\"\n",
    "    \n",
    "If the input is to do with retail or an item to buy, do the following:\n",
    "\n",
    "Use the retriever tool to answer any queries related to internal data files. \n",
    "Only use external tools (like Wikipedia) if no relevant information is found internally.\n",
    "When asked about a specific item output the answers to the following queries: \n",
    "\n",
    "Should this item be ordered? \n",
    "Find out what date it is currently using date time tool. Convert the month and year into strings and use these for the rest of the answer.\n",
    "Use recent stock statistics to see if the stocks for this month are low compared to how much was used last month.\n",
    "Use the statistics from last year to predict how much of each item will be needed for the rest of this month and next month.\n",
    "\n",
    "Use wikipedia tool to find out whether any important holidays eg christmas or summer will be coming up and mention this search.\n",
    "Use wikipedia search information and the date information in your decisions.\n",
    "a) how much should be ordered for this month and next month\n",
    "b) how much should be ordered in the next few months\n",
    "\n",
    "What brand?\n",
    "Use the customers reviews to decide this. \n",
    "\n",
    "If this item should be ordered, how much of this item should be ordered?\n",
    "Use the repl tool to calculate these numbers.\n",
    "\n",
    "In your final output:\n",
    "1. Explain your reasoning for whether an item should be ordered and how much.\n",
    "Don't explain your reasoning for\n",
    "Explain the maths you used.\n",
    "2. Draft an email to an imaginary supplier of this brand to order the number of the item you think should be ordered, signing off as Reply Auto Replenishment.\n",
    "\n",
    "Else:\n",
    "Answer the question appropriately using multiple tools to verify your answer\n",
    "\n",
    "Query: {input}\n",
    "\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e6f16a5-3256-4ece-a377-bdb6132de3fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# math_prompt = PromptTemplate(\n",
    "#     input_variables=[\"input\"],\n",
    "#     template=\"Calculate the result of the following expression and return it as a valid Python expression: {input}\"\n",
    "# )\n",
    "# math_tool = load_tools([\"llm-math\"], llm=bedrock_llm,prompt=math_prompt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74b3f7f5-eccf-423f-b968-117620b8243c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#datetime tool\n",
    "from datetime import datetime\n",
    "datetime_tool = Tool(\n",
    "    name=\"Datetime\",\n",
    "    func=lambda x: datetime.now().isoformat(),\n",
    "    description=\"Returns the current datetime\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ec71f01-e204-4492-ba9e-462271b381d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#loading in tools\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "repl_tool = Tool(\n",
    "    name=\"python_repl\",\n",
    "    description=\"Use this to execute python commands and calculations.\",\n",
    "    func=python_repl.run,\n",
    ")\n",
    "wikipedia_tool = load_tools([\"wikipedia\"], llm=bedrock_llm)\n",
    "\n",
    "tools = [datetime_tool,retriever_tool,search,repl_tool,wikipedia_tool[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84811c9c-0cb3-4590-bc6e-14aa8379fb03",
   "metadata": {
    "tags": []
   },
   "source": [
    "## customer agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "470ad0c7-0e4c-4c79-b84b-556d420fc5f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"agent_scratchpad\"],  # Include both input and agent_scratchpad\n",
    "    template=\"\"\"\n",
    "    whatever item is inputted, return three clients's Ids that it would be best to advertise this item to.\n",
    "    use retriever tool to look through list of clients in supermarket.csv file.\n",
    "    Return their client ids, found in the first column on the csv file.\n",
    "    \n",
    "    using tavilly search / wikipedia / repl tool do the following:\n",
    "    identify what the item is, what is made of, and benefits from this product if applicable.\n",
    "    come up with the best way to advertise it to them - using factors such as their name and their spending category.\n",
    "    give clients that have shopped at the company for longer a greater discount and mention this fact.\n",
    "Query: {input}\n",
    "\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42582383-482a-416a-b05a-0371adfd3d51",
   "metadata": {
    "tags": []
   },
   "source": [
    "## running agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25bca77f-5863-42c0-84b6-479e667c4e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_agent(prompt,agent_input,raw_list,verbose_setting):\n",
    "    agent = create_tool_calling_agent(bedrock_llm, tools, prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=verbose_setting,handle_parsing_errors=True)\n",
    "    response = agent_executor.invoke({\"input\": agent_input})\n",
    "    raw_output = response['output'][0]['text']\n",
    "    # raw_list.append(raw_output)\n",
    "    print(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d874337c-616a-4d26-a304-c052e8d602b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Python REPL can execute arbitrary code. Use with caution.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is my analysis and recommendation for ordering Christmas pudding:\n",
      "\n",
      "Based on the internal data, Christmas pudding is a seasonal item that is only sold in December. Last December, a total of 20 units were sold across the three brands (Brand A: 12 units, Brand B: 8 units, Brand C: 0 units). \n",
      "\n",
      "Looking at this month (December 2024), we only have 2 units left in stock, while last month (November) we used 5 units. Since the stock left this month is less than 20% of what was used last month, our stocks are considered low for this seasonal item.\n",
      "\n",
      "To predict our needs for this month and next, I looked at sales data from last year. In December 2023, we sold 20 units total. In January 2024, we sold 0 units. So I will predict we need 20 units for this month (December 2024) and 0 units for next month (January 2025).\n",
      "\n",
      "Given we only have 2 units left, I recommend ordering an additional 18 units for this month to meet the predicted demand of 20 units. For next month, no additional ordering is needed since demand is predicted to be 0.\n",
      "\n",
      "When choosing which brand to order, I analyzed the customer review data:\n",
      "\n",
      "Brand A reviews: [\"Not sure I'd buy it again.\", 'Could have been better in some ways.', 'A bit disappointing compared to others.', 'Not quite what I hoped for.', 'Had some issues, not thrilled.']\n",
      "Brand B reviews: ['Decent for the price.', 'It was okay, nothing special.', 'Met my expectations, no more, no less.', 'An average experience overall.', 'Not bad, but could improve.'] \n",
      "Brand C reviews: ['An average experience overall.', 'Decent for the price.', 'It was okay, nothing special.', 'Met my expectations, no more, no less.', 'Not bad, but could improve.']\n",
      "\n",
      "The reviews for Brand A seem more negative overall compared to Brands B and C. Brand B and C have more neutral/average reviews. So I would recommend ordering Brand B or Brand C over Brand A.\n",
      "\n",
      "I also checked if any major holidays are coming up using Wikipedia, which could impact demand. Christmas is on December 25th, so that explains the high seasonal demand we see for Christmas pudding in December. No other major holidays were found in the next couple months.\n",
      "\n",
      "To summarize my recommendations:\n",
      "- Order 18 additional units of Christmas pudding for this month (December 2024)\n",
      "- Do not order any for next month (January 2025) \n",
      "- Choose either Brand B or Brand C based on the more positive customer reviews\n",
      "- Be prepared for high demand this month due to the Christmas holiday\n",
      "\n",
      "Draft email to supplier:\n",
      "\n",
      "Subject: Christmas Pudding Order for December 2024\n",
      "\n",
      "Dear Supplier,\n",
      "\n",
      "I am writing to place an order for 18 units of your [Brand B/Brand C] Christmas pudding for delivery this month (December 2024). Based on sales data from previous years and our current low stock levels, we anticipate this amount will be needed to meet customer demand over the Christmas holiday period.  \n",
      "\n",
      "Please process this order as soon as possible to ensure we have sufficient inventory for the holiday rush. We appreciate your prompt assistance with this seasonal replenishment.\n",
      "\n",
      "Thank you,\n",
      "Reply Auto Replenishment\n"
     ]
    }
   ],
   "source": [
    "raw_list = []\n",
    "#run_agent(prompt,'suncream',raw_list,False)\n",
    "run_agent(prompt,'christmas pudding',raw_list,False)\n",
    "#run_agent(customer_prompt,'cereal',raw_list,False)\n",
    "#run_agent(customer_prompt,'bread',raw_list,False)\n",
    "\n",
    "# for i in range(len(raw_list)):\n",
    "#     print('-'*20)\n",
    "#     print(raw_list[i])\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a910cfc-5920-4b27-8fbd-ee516c45cc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# customer_agent = create_tool_calling_agent(bedrock_llm, tools, prompt)\n",
    "# agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True,handle_parsing_errors=True)\n",
    "# response = agent_executor.invoke({\"input\": 'sun cream'})\n",
    "# raw_output = response['output'][0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "726531fe-a19f-4684-825d-6bedd1f83519",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# agent = create_tool_calling_agent(bedrock_llm, tools, prompt)\n",
    "# agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True,handle_parsing_errors=True)\n",
    "# response = agent_executor.invoke({\"input\": 'sun cream'})\n",
    "# raw_output = response['output'][0]['text']\n",
    "\n",
    "# print(raw_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de00504-0586-4ad2-8009-e316a68969cf",
   "metadata": {},
   "source": [
    "## multi agent system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e75ee6-8bdd-4a0d-aaf7-c5d630ce2caf",
   "metadata": {
    "tags": []
   },
   "source": [
    "### creating agent supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b42d913-457a-4137-9b79-7f4389b561da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "members = [\"researcher\", \"communication\"]\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = members + [\"FINISH\"]\n",
    "\n",
    "system_prompt = f\"\"\"You are a supervisor tasked with managing a conversation between the\n",
    "    following workers: {members}. Given the following user request,\n",
    "    respond with the worker to act next. Each worker will perform a\n",
    "    task and respond with their results and status. \n",
    "    \n",
    "    Do not let the workers attempt the task more than 10 times, stop them before the recursive limit of 25 is reached.\n",
    "    \n",
    "    When a good email has been written by communication node, stop the workers,\n",
    "    respond with FINISH, and output the final email.\"\"\"\n",
    "\n",
    "class Router(TypedDict):\n",
    "    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "\n",
    "    next: Literal[options]\n",
    "\n",
    "\n",
    "def supervisor_node(state: MessagesState) -> Command[Literal[members, \"__end__\"]]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ] + state[\"messages\"]\n",
    "    response = bedrock_llm.with_structured_output(Router).invoke(messages)\n",
    "    goto = response[\"next\"]\n",
    "    if goto == \"FINISH\":\n",
    "        goto = END\n",
    "    \n",
    "    print('hello',response)\n",
    "    # return Command(goto=goto)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1186f440-c153-4b93-ab76-67014b002a44",
   "metadata": {
    "tags": []
   },
   "source": [
    "### construct graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28b275b1-7452-4a14-bb26-6a7a8b6c56ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during graph compilation: unhashable type: 'list'\n",
      "Node: supervisor, Ends: (['researcher', 'communication'], '__end__')\n",
      "Node: researcher, Ends: ('supervisor',)\n",
      "Node: communication, Ends: ('supervisor',)\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Define agents\n",
    "# research_agent = create_react_agent(\n",
    "#     bedrock_llm, tools=tools, state_modifier=\"You are a researcher. You do the maths and predictions.\"\n",
    "# )\n",
    "\n",
    "research_string = \"\"\"\n",
    "    \n",
    "If the input is to do with retail or an item to buy, do the following:\n",
    "\n",
    "Use the retriever tool to answer any queries related to internal data files. \n",
    "Only use external tools (like Wikipedia) if no relevant information is found internally.\n",
    "When asked about a specific item output the answers to the following queries: \n",
    "\n",
    "Should this item be ordered? \n",
    "Find out what date it is currently using date time tool. Convert the month and year into strings and use these for the rest of the answer.\n",
    "Use recent stock statistics to see if the stocks for this month are low compared to how much was used last month.\n",
    "Use the statistics from last year to predict how much of each item will be needed for the rest of this month and next month.\n",
    "Use what month it is at the time the call is being made and whether any important holidays eg christmas or summer will be coming up and use this information in your decision.\n",
    "a) how much should be ordered for this month and next month\n",
    "b) how much should be ordered in the next few months\n",
    "\n",
    "\n",
    "What brand?\n",
    "Use the customers reviews to decide this. \n",
    "\n",
    "If this item should be ordered, how much of this item should be ordered?\n",
    "Use the repl tool to calculate these numbers.\n",
    "\n",
    "Query: {input}\n",
    "\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "communication_string = \"\"\"\n",
    "    \n",
    "Using information from the research node, \n",
    "Draft an email to an imaginary supplier of this brand to order the number of the item you think should be ordered, \n",
    "signing off as Reply Auto Replenishment.\n",
    "\n",
    "Query: {input}\n",
    "\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "research_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"agent_scratchpad\"],  # Include both input and agent_scratchpad\n",
    "    template=research_string\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "communication_prompt= PromptTemplate(\n",
    "    input_variables=[\"input\", \"agent_scratchpad\"],  # Include both input and agent_scratchpad\n",
    "    template=communication_string\n",
    ")\n",
    "\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    bedrock_llm, tools=tools, state_modifier=research_string\n",
    ")\n",
    "\n",
    "def research_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "# Communication node definition\n",
    "communication_agent = create_react_agent(bedrock_llm, tools=[search,wikipedia_tool[0]],state_modifier=communication_string)\n",
    "\n",
    "def communication_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = communication_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"communication\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "# Supervisor node definition\n",
    "# def supervisor_node(state: MessagesState) -> Command[Literal[\"researcher\", \"communication\", END]]:\n",
    "#     # Dummy implementation of supervisor logic\n",
    "#     return Command(update={}, goto=\"researcher\")  # Example logic\n",
    "\n",
    "# Build StateGraph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes explicitly with corresponding functions\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"researcher\", research_node)\n",
    "builder.add_node(\"communication\", communication_node)\n",
    "\n",
    "# Add edges explicitly\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "builder.add_edge(\"supervisor\", \"researcher\")\n",
    "builder.add_edge(\"researcher\", \"supervisor\")\n",
    "builder.add_edge(\"supervisor\", \"communication\")\n",
    "builder.add_edge(\"communication\", \"supervisor\")\n",
    "builder.add_edge(\"supervisor\", END)\n",
    "\n",
    "# Compile graph\n",
    "try:\n",
    "    graph = builder.compile()\n",
    "except TypeError as e:\n",
    "    print(\"Error during graph compilation:\", e)\n",
    "    # Debugging spec.ends if needed\n",
    "    for node_name, spec in builder.nodes.items():\n",
    "        print(f\"Node: {node_name}, Ends: {spec.ends}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed3cb09e-41ca-4315-90ba-6bcbcaa2df35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, Image\n\u001b[0;32m----> 3\u001b[0m display(Image(\u001b[43mgraph\u001b[49m\u001b[38;5;241m.\u001b[39mget_graph()\u001b[38;5;241m.\u001b[39mdraw_mermaid_png()))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'graph' is not defined"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba8ba30-6fd8-4bb3-ad38-49edd0197b6b",
   "metadata": {},
   "source": [
    "### invoke team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8aff83-a6ab-4e2d-a67d-456e86b239bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for s in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"Bread\")]}, subgraphs=True\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec078f-a7b5-407a-9932-7f8a6bf51451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from langgraph.types import Command\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# Define team members and options\n",
    "members = [\"researcher\", \"communication\"]\n",
    "options = members + [\"FINISH\"]\n",
    "\n",
    "# Supervisor system prompt\n",
    "system_prompt = f\"\"\"You are a supervisor managing a conversation between the workers: {members}. \n",
    "You decide which worker should act next based on the current task progress. \n",
    "Stop all work and respond with FINISH when a good email has been written by the communication node. \n",
    "Output the final email when the task is complete.\"\"\"\n",
    "\n",
    "# Supervisor routing logic\n",
    "class Router(TypedDict):\n",
    "    next: Literal[options]\n",
    "\n",
    "def supervisor_node(state: MessagesState) -> Command[Literal[members + [END]]]:\n",
    "    messages = state[\"messages\"]\n",
    "    # Check if email is complete\n",
    "    for msg in messages[::-1]:\n",
    "        if \"email completed\" in msg.get(\"content\", \"\").lower():\n",
    "            final_email = msg[\"content\"]\n",
    "            return Command(update={\"final_email\": final_email}, goto=END)\n",
    "    # Decide next step based on the last message sender\n",
    "    last_sender = messages[-1].get(\"name\", \"\") if messages else \"\"\n",
    "    if last_sender == \"researcher\":\n",
    "        return Command(update={}, goto=\"communication\")\n",
    "    elif last_sender == \"communication\":\n",
    "        return Command(update={}, goto=\"researcher\")\n",
    "    return Command(update={}, goto=\"researcher\")  # Default to researcher\n",
    "\n",
    "# Researcher prompt\n",
    "research_string = \"\"\"\n",
    "If the input is to do with retail or an item to buy, do the following:\n",
    "\n",
    "Use the retriever tool to answer any queries related to internal data files. \n",
    "Only use external tools (like Wikipedia) if no relevant information is found internally.\n",
    "When asked about a specific item output the answers to the following queries: \n",
    "\n",
    "Should this item be ordered? \n",
    "Find out what date it is currently using date time tool. Convert the month and year into strings and use these for the rest of the answer.\n",
    "Use recent stock statistics to see if the stocks for this month are low compared to how much was used last month.\n",
    "Use the statistics from last year to predict how much of each item will be needed for the rest of this month and next month.\n",
    "Use what month it is at the time the call is being made and whether any important holidays eg christmas or summer will be coming up and use this information in your decision.\n",
    "a) how much should be ordered for this month and next month\n",
    "b) how much should be ordered in the next few months\n",
    "\n",
    "What brand?\n",
    "Use the customers reviews to decide this. \n",
    "\n",
    "If this item should be ordered, how much of this item should be ordered?\n",
    "Use the repl tool to calculate these numbers.\n",
    "\n",
    "Query: {input}\n",
    "\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "research_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"agent_scratchpad\"], template=research_string\n",
    ")\n",
    "\n",
    "# Researcher node\n",
    "def research_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    research_agent = create_react_agent(bedrock_llm, tools=tools, state_modifier=research_string)\n",
    "    result = research_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "# Communication prompt\n",
    "communication_string = \"\"\"\n",
    "Using the information from the researcher, draft an email to a supplier for ordering items.\n",
    "Sign off as Reply Auto Replenishment.\n",
    "\n",
    "Query: {input}\n",
    "\n",
    "{agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "communication_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"agent_scratchpad\"], template=communication_string\n",
    ")\n",
    "\n",
    "# Communication node\n",
    "def communication_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    communication_agent = create_react_agent(bedrock_llm, tools=[search, wikipedia_tool[0]], state_modifier=communication_string)\n",
    "    result = communication_agent.invoke(state)\n",
    "    content = result[\"messages\"][-1].content\n",
    "    if \"good email\" in content.lower():\n",
    "        content += \"\\nStatus: Email completed.\"\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=content, name=\"communication\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "# Build the StateGraph\n",
    "builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"researcher\", research_node)\n",
    "builder.add_node(\"communication\", communication_node)\n",
    "\n",
    "# Add edges\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "builder.add_edge(\"supervisor\", \"researcher\")\n",
    "builder.add_edge(\"researcher\", \"supervisor\")\n",
    "builder.add_edge(\"supervisor\", \"communication\")\n",
    "builder.add_edge(\"communication\", \"supervisor\")\n",
    "builder.add_edge(\"supervisor\", END)\n",
    "\n",
    "# Compile the graph\n",
    "try:\n",
    "    graph = builder.compile()\n",
    "except TypeError as e:\n",
    "    print(\"Error during graph compilation:\", e)\n",
    "\n",
    "# Stream execution\n",
    "for s in graph.stream(\n",
    "    {\"messages\": [(\"user\", \"Bread\")]}, subgraphs=True\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3486a58f-e512-4ea5-99f1-372b919d3ea9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## making own math tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5fc3e6-cad8-47da-a45e-fd38ee25b5ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import LLMMathChain, LLMChain\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "wikipedia_tool = Tool(name=\"Wikipedia\",\n",
    " func=wikipedia.run,\n",
    " description=\"A useful tool for searching the Internet\n",
    "\n",
    "word_problem_template = \"\"\"You are a reasoning agent tasked with solving \n",
    "the user's logic-based questions. Logically arrive at the solution, and be \n",
    "factual. In your answers, clearly detail the steps involved and give the \n",
    "final answer. Provide the response in bullet points. \n",
    "Question  {question} Answer\"\"\"\n",
    "\n",
    "math_assistant_prompt = PromptTemplate(input_variables=[\"question\"],\n",
    "                                       template=word_problem_template\n",
    "                                       )\n",
    "\n",
    "word_problem_chain = LLMChain(llm=bedrock_llm,\n",
    "                              prompt=math_assistant_prompt)\n",
    "\n",
    "word_problem_tool = Tool.from_function(name=\"Reasoning Tool\",\n",
    "                                       func=word_problem_chain.run,\n",
    "                                       description=\"Useful for when you need to answer logic-based/reasoning questions.\",\n",
    " )\n",
    "\n",
    "\n",
    "problem_chain = LLMMathChain.from_llm(llm=bedrock_llm)\n",
    "\n",
    "math_tool = Tool.from_function(name=\"Calculator\",\n",
    "    func=problem_chain.run,\n",
    "    description=\"Useful for when you need to answer questions about math. This tool is only for math questions and nothing else. Only input math expressions.\")\n",
    "\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[wikipedia_tool[0], math_tool, word_problem_tool],\n",
    "    llm=bedrock_llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=False,\n",
    "    handle_parsing_errors=True\n",
    " )\n",
    "\n",
    "print(agent.invoke({\"input\": \"\"\"I have 3 apples and 4 oranges. \n",
    "     I give half of my oranges away and buy two dozen new ones, along\n",
    "     with three packs of strawberries. Each pack of strawberry has 30 strawberries.\n",
    "     How  many total pieces of fruit do I have at the end?\"\"\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e4246-96b2-4c42-ae5b-0c45fe289fd2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## csv writing agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551923c-7c39-4c81-a06f-5fd268de1a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_experimental.agents.agent_toolkits import create_csv_agent\n",
    "\n",
    "csv_agent = create_csv_agent(\n",
    "    llm,\n",
    "    \"draft_emails_agent.csv\",\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3ba19-839a-41b9-8187-063f40813ed9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5230bb-43b9-43d9-8427-fdfbc7d4459a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm=bedrock_llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "#result = agent.invoke({'input':\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"})\n",
    "inputs = {\n",
    "    \"page_content\": \"Content from the document\",\n",
    "    \"input\": \"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\"\n",
    "}\n",
    "result = agent.invoke(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c317dc-6fbb-4ba3-af40-1c9c7c09f8bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "math_query = \"What is the square root of 256 multiplied by 10?\"\n",
    "mq2=\"What was the high temperature in SF yesterday in Fahrenheit? What is that number raised to the .023 power?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d04ee81-f5c8-4c7c-b92f-3c497b188851",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({\"input\": '3+3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3c2042-390a-46f5-bcdf-dd81952e26dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## call agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41ed9b4-cea4-469f-8573-760df4fb6b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "math = agent_executor.invoke({'input':'what is 13 +20'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91108855-ee0c-4b5f-9fce-448132141b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = agent_executor.invoke({\"input\": \"Toothpaste\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bec681-95c5-4068-b705-f6f4aba18ad4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "# Extract the raw text output\n",
    "raw_output = response['output'][0]['text']\n",
    "\n",
    "print(raw_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d532b6-7588-4685-a893-f80f64c1b38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the output (example formatting logic)\n",
    "def format_output(raw_text):\n",
    "    # Parse raw text to extract relevant sections (if structured)\n",
    "    lines = raw_text.split(\"\\n\")\n",
    "    formatted_text = dedent(f\"\"\"\n",
    "    **Order Summary**\n",
    "\n",
    "    1. Should this item be ordered?\n",
    "       {lines[3]}\n",
    "\n",
    "    2. **Recommended Brand:**\n",
    "       {lines[4]}\n",
    "       - Review Highlights:\n",
    "           - {lines[5]}\n",
    "           - {lines[6]}\n",
    "\n",
    "    **Draft Email to Supplier**\n",
    "    {lines[-2]}\n",
    "\n",
    "    **Next Steps:**\n",
    "    1. Confirm availability with the supplier.\n",
    "    2. Place the order.\n",
    "    3. Follow up on delivery.\n",
    "    \"\"\")\n",
    "    return formatted_text\n",
    "\n",
    "# Format the response\n",
    "formatted_response = format_output(raw_output)\n",
    "\n",
    "# Display the formatted response\n",
    "print(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d67a8-4a43-4670-8266-fefd74da0f88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
